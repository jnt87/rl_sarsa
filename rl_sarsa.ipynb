{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuZ0PbpvfemR"
   },
   "source": [
    "## 1 Description\n",
    "\n",
    "For this assignment, you will build a Sarsa agent which will learn policies in the OpenAI Gym [Frozen Lake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) environment. [OpenAI Gym](http://gym.openai.com/docs/) is a platform where users can test their RL algorithms on a selection of carefully crafted environments.\n",
    "\n",
    "Frozen Lake is a grid world environment that is highly stochastic, where the agent must cross a slippery frozen lake which has deadly holes to fall through. The agent begins in the starting state $S$ and is given a reward of 1 if it reaches the goal state $G$. A reward of 0 is given for all other transitions.\n",
    "\n",
    "The agent can take one of four possible moves at each state (left, down, right, or up). The frozen cells $F$ are slippery, so the agentâ€™s actions succeed only $\\frac{1}{3}$ of the time, while the other $\\frac{2}{3}$ are split evenly between the two directions orthogonal to the intended direction. If the agent lands in a hole $H$, then the episode terminates.\n",
    "\n",
    "You will be given a randomized Frozen Lake map with a corresponding set of parameters to train your Sarsa agent with.  \n",
    "If your agent is implemented correctly, then after training it for the specified number of episodes, \n",
    "your agent will produce the same policy (not necessarily an optimal policy).\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/cykbkk5mg5652n9/frozen_lake.png?raw=1\" width=200px;>\n",
    "\n",
    "Figure 1: Example Frozen Lake Map\n",
    "\n",
    "The toturials of Fronzen_Lake.   [Frozen Lake env](https://reinforcement-learning4.fun/2019/06/16/gym-tutorial-frozen-lake/), [gym](https://gym.openai.com/envs/FrozenLake-v0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ehl1IfON6U4J"
   },
   "source": [
    "## 2 Sarsa $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "\n",
    "Sarsa uses temporal-difference learning to form a model-free on-policy reinforcement-learning algorithm that solves the \\textit{control} problem.  It is model-free because it does not need and does not use a model of the environment, namely neither a transition nor reward function; \n",
    "instead, Sarsa samples transitions and rewards online.\n",
    "\n",
    "It is on-policy because it learns about the same policy that generates its behaviors (this is in contrast to Q-learning, which you'll examine in your next homework).  That is, Sarsa estimates the action-value function of its behavior policy. In this homework, you will not be training a Sarsa agent \n",
    "to approximate the \\textit{optimal} action-value function; instead, the hyperparameters of both the \n",
    "exploration strategy and the algorithm will be given to you as input\\textemdash the goal being to verify that your SARSA agent is correctly implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_LhCNN-6hwZ"
   },
   "source": [
    "## 3 Procedure\n",
    "\n",
    "Since this homework requires you to match a non-deterministic output between your\n",
    "agent and the autograder's agent, attention to detail to each of the following points \n",
    "is required:\n",
    "\n",
    "- You must use Python and the library NumPy for this homework\n",
    "- Install OpenAI Gym (e.g. `pip install gym`).\n",
    "- Instantiate the Frozen Lake environment with `gym.make(FrozenLake-v0, desc=desc).unwrapped`, where the attribute desc is a square version of the string input **amap**. This means you must resize **amap** so that it looks like a square grid, where the input characters fill in the grid row by row.  Make sure you use the unwrapped method as indicated above.\n",
    "- The input **amap** should be used to seed the random number generators for both Gym and NumPy.  Do *not* use the Python standard library's random library.\n",
    "- Implement your Sarsa agent using an $\\epsilon$-greedy behavioral policy.  Specifically, you must use `numpy.random.random` to choose whether or not the action is greedy, and `numpy.random.randint` to select the random action.\n",
    "- Initialize the agentâ€™s Q-table to zeros.\n",
    "- Train your agent using the given input parameters.  The input `amap` is the Frozen Lake map that you need to resize and provide to the `desc` attribute when you instantiate your environment.  The input `gamma` is the discount rate. The input `alpha` is the learning rate.  The input `epsilon` is the parameter for the $\\epsilon$-greedy behavior strategy your Sarsa agent will use.  Specifically, an action should be selected uniformly at random if a random number drawn uniformly between $0$ and $1$ is less than $\\epsilon$.  If the greedy action is selected, the action with lowest index should be selected in case of ties.  The input `n_episodes` is the number of episodes to train your agent.  Finally, `seed` is the number used to seed both Gym's random number generator `and` NumPy's random number generator.\n",
    "- To sync with the autograder, your Sarsa implementation should select the action corresponding to the next state the agent will visit *even when* that next state is a terminal state (this action will never be executed by the agent).\n",
    "- The answer you provide should be the greedy policy with respect to the Q-function obtained by your Sarsa agent after the completion of the final episode. Specifically, the policy should be expressed as a string of characters: `<, v, >, ^`, representing left, down, right, and up, respectively.  The ordering of the actions in the output should reflect the ordering of states in **amap**.  Provide answers for the specific problems you are given on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HD1Hpet8_e9"
   },
   "source": [
    "## 4 Examples\n",
    "\n",
    "The following examples can be used to verify that your agent is implemented correctly.\n",
    "- Input: `amap=\"SFFG\"; gamma=1.0; alpha=0.24; epsilon=0.09; n_episodes=49553; seed=202404`\n",
    "    \n",
    "  Output: `<<v<`\n",
    "  \n",
    "- Input: `amap=\"SFFFHFFFFFFFFFFG\"; gamma=1.0; alpha=0.25; epsilon=0.29; n_episodes=14697; seed=741684`\n",
    "    \n",
    "  Output: `^vv><>>vvv>v>>><`\n",
    "\n",
    "- Input: `amap=\"SFFFFHFFFFFFFFFFFFFFFFFFG\"; gamma=0.91; alpha=0.12; epsilon=0.13; n_episodes=42271; seed=983459`\n",
    "    \n",
    "  Output: `^>>>><>>>vvv>>vv>>>>v>>^<`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U14IfY6HHYcc"
   },
   "source": [
    "## 5 Implementation\n",
    "\n",
    "We provide some start code for reference. Feel free to modify it. You are required to finish the TODO parts to let the agent run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXavHyQPfjpX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic_control] in /home/jmint/miniconda3/envs/jupyter-env/lib/python3.11/site-packages (1.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/jmint/miniconda3/envs/jupyter-env/lib/python3.11/site-packages (from gymnasium[classic_control]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jmint/miniconda3/envs/jupyter-env/lib/python3.11/site-packages (from gymnasium[classic_control]) (3.1.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/jmint/miniconda3/envs/jupyter-env/lib/python3.11/site-packages (from gymnasium[classic_control]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/jmint/miniconda3/envs/jupyter-env/lib/python3.11/site-packages (from gymnasium[classic_control]) (0.0.4)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/jmint/miniconda3/envs/jupyter-env/lib/python3.11/site-packages (from gymnasium[classic_control]) (2.6.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[classic_control]\n",
    "\n",
    "# Environment\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
    "\n",
    "# initialzation\n",
    "MOVES = {\n",
    "    0: \"<\",\n",
    "    1: \"v\",\n",
    "    2: \">\",\n",
    "    3: \"^\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpsV4-gWKGgr"
   },
   "outputs": [],
   "source": [
    "# Agent\n",
    "\n",
    "def solve(env, gamma, alpha, epsilon, n_episodes, seed):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: Fronzen_Lake environment. See Fronzen Lake env link in section 1 about toturials of Fronzen_Lake.\n",
    "        gamma: discount rate\n",
    "        alpha : learning rate\n",
    "        epsilon : parameter for the  ðœ– -greedy behavior\n",
    "        n_episodes : the number of episodes to train your agent\n",
    "        seed : the number used to seed both Gymâ€™s random number generator and NumPyâ€™s random number generator\n",
    "    Return:\n",
    "        Don't need to return anything, just fill out the TODOs.\n",
    "    \"\"\"\n",
    "    # Note: please see how to define the env is the following Test Example.\n",
    "\n",
    "    # TODO: Set seed\n",
    "    # Hint: use np.random.seed and env.seed\n",
    "    np.random.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    \n",
    "    # TODO: Initialize the agentâ€™s Q-table to zeros. \n",
    "    # Hint: use np.zeros with env.observation_space.n and env.action_space.n\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        # define state\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        done = False\n",
    "        # TODO: define action\n",
    "        # consider two cases: np.random.random() > epsilon or not\n",
    "        if np.random.random() > epsilon:\n",
    "            # Exploitation: choose best action\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            # Exploration: pick random action\n",
    "            action = np.random.randint(0, 4)\n",
    "        \n",
    "        # Update state and action\n",
    "        while not done:\n",
    "            # TODO: get new_state, reward, done \n",
    "            # Hint: use env.step()\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # TODO: get new_action\n",
    "            if np.random.random() > epsilon:\n",
    "                new_action = np.argmax(Q[new_state])\n",
    "            else:\n",
    "                new_action = np.random.randint(0, 4)\n",
    "                \n",
    "            # TODO: Q-function\n",
    "            Q[state, action] += alpha * (reward + gamma + Q[new_state, new_action] - Q[state, action])\n",
    "\n",
    "            # update state and action\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "    # print the result        \n",
    "    print(\",\".join([MOVES[np.argmax(i)] for i in Q]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wAi1swlflUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: SF\n",
      "Row 1: FG\n",
      "^,<,^,<\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "amap = \"SFFG\"\n",
    "amap_row = 2\n",
    "amap_col = 2\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\",\n",
    "    desc = np.reshape(list(amap), (amap_row,amap_col)),\n",
    "    map_name=None,\n",
    "    is_slippery=False,\n",
    "    render_mode=\"ansi\"\n",
    ").unwrapped\n",
    "\n",
    "for idx, row in enumerate(env.unwrapped.desc):\n",
    "    print(f\"Row {idx}: {''.join(row.astype(str))}\")\n",
    "\n",
    "seed=202404\n",
    "gamma=1.0\n",
    "alpha=0.24\n",
    "epsilon=0.09\n",
    "n_episodes=49553\n",
    "\n",
    "solve(env, gamma, alpha, epsilon, n_episodes, seed)\n",
    "\n",
    "# expected output: <,<,v,<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
